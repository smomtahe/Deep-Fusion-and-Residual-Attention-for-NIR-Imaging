{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smomtahe/Project_BreastCancer_Advanced_DeepLearnig/blob/main/2_Images_CNN_Transformer_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Images are used and trained in the dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, UpSampling2D, concatenate, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Load dataset\n",
        "dataset_df = pd.read_csv(\"/content/dataImagesSum.csv\")\n",
        "\n",
        "# Extract features (assuming they are in the first 128 columns)\n",
        "X = dataset_df.iloc[:, 2:130].values\n",
        "image_paths = dataset_df['path'].values  # Path column\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/images.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extracted_dir_path = '/content/images/'\n",
        "\n",
        "# Create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall('/content')\n",
        "\n",
        "print(\"Extraction complete. The files are extracted to:\", extracted_dir_path)\n"
      ],
      "metadata": {
        "id": "kOoADuCQz8JN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fb38b3-3a52-4402-8107-1892fcb356aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete. The files are extracted to: /content/images/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess images\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)  # Preprocess based on the model you choose\n",
        "    return img\n",
        "\n",
        "# Preprocess all images and stack them into a single numpy array\n",
        "preprocessed_images = np.vstack([preprocess_image(os.path.join(extracted_dir_path, path)) for path in image_paths])\n"
      ],
      "metadata": {
        "id": "c4h0nRpcfCvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and targets (Y)\n",
        "# Setting the first 12 samples as the test set\n",
        "X_test = X[:12, :]\n",
        "y_test = preprocessed_images[:12]\n",
        "\n",
        "# Using the remaining samples as the training set\n",
        "X_train = X[12:, :]\n",
        "y_train = preprocessed_images[12:]\n"
      ],
      "metadata": {
        "id": "yqyVpJT4goGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Assuming X contains numerical features\n",
        "X = dataset_df.iloc[:, 2:130].values\n",
        "\n",
        "# Assuming y contains the image data\n",
        "y = dataset_df.iloc[:, 130:].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define simple model architecture\n",
        "input_features = Input(shape=(128,))\n",
        "x = Dense(256, activation='relu')(input_features)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output_image = Dense(128, activation='sigmoid')(x)  # Adjusted output size to match the shape of y_train\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_features, outputs=output_image)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "\n",
        "# Predict images from the test set features\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"a=[{', '.join(map(str, y_pred[4]))}];\")\n",
        "print(f\"aa=[{', '.join(map(str, y_pred[5]))}];\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "y3kf5-6nzcFf",
        "outputId": "a9af60ae-b4bc-406d-ced0-27bd23d46069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 147ms/step - loss: 0.2281\n",
            "Test Loss: 0.22806069254875183\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "a=[0.4628131, 0.5822667, 0.5120999, 0.47373757, 0.4990976, 0.52178276, 0.5539396, 0.49500045, 0.49975592, 0.45893002, 0.48761225, 0.37973127, 0.52434266, 0.48505893, 0.44264328, 0.48347217, 0.46523833, 0.47951674, 0.5423786, 0.46079236, 0.538118, 0.46935773, 0.553081, 0.45364383, 0.47372285, 0.5545743, 0.43659794, 0.53957933, 0.53168136, 0.47887823, 0.533823, 0.49411845, 0.4810281, 0.45199138, 0.50742674, 0.45498362, 0.46633697, 0.5745793, 0.4874386, 0.5018254, 0.4652223, 0.437341, 0.56159896, 0.42294347, 0.47027233, 0.4312053, 0.52868533, 0.51716745, 0.5485312, 0.4601754, 0.5746321, 0.6049049, 0.48545945, 0.47573903, 0.5244517, 0.44996837, 0.4905842, 0.5023082, 0.5321399, 0.50742084, 0.4591343, 0.5196516, 0.5442181, 0.52072334, 0.4783967, 0.47118306, 0.55391234, 0.42384055, 0.45152915, 0.4255721, 0.48124248, 0.57895446, 0.4197661, 0.5241581, 0.48642546, 0.50970817, 0.47934845, 0.47607577, 0.43536416, 0.5240661, 0.44650328, 0.5194766, 0.5413431, 0.46222833, 0.49193776, 0.50289124, 0.5659268, 0.46826604, 0.47281396, 0.5087273, 0.55012536, 0.58146983, 0.5682965, 0.57021856, 0.5095317, 0.49692398, 0.48490798, 0.44361192, 0.5589788, 0.4965124, 0.5321791, 0.4308208, 0.45140567, 0.45940277, 0.60525054, 0.4890814, 0.5101561, 0.47334346, 0.51018155, 0.5764678, 0.5161302, 0.4469753, 0.4805908, 0.45510465, 0.48374367, 0.52466387, 0.47000673, 0.45970276, 0.47487703, 0.53118587, 0.5640656, 0.46688598, 0.4980134, 0.46584907, 0.4833342, 0.5406599, 0.45751318, 0.4681322];\n",
            "aa=[0.49999928, 0.5000019, 0.50000054, 0.49999967, 0.4999999, 0.50000024, 0.50000113, 0.4999999, 0.5, 0.49999914, 0.4999996, 0.49999723, 0.50000054, 0.49999973, 0.49999866, 0.49999967, 0.49999896, 0.49999973, 0.50000113, 0.49999917, 0.5000005, 0.4999992, 0.50000083, 0.49999902, 0.49999937, 0.5000009, 0.4999983, 0.500001, 0.50000054, 0.4999997, 0.500001, 0.4999998, 0.49999958, 0.49999902, 0.5000002, 0.49999925, 0.4999993, 0.50000185, 0.49999955, 0.50000006, 0.4999992, 0.49999854, 0.500001, 0.49999836, 0.49999937, 0.49999818, 0.50000054, 0.5000004, 0.500001, 0.4999989, 0.5000017, 0.50000256, 0.49999982, 0.4999997, 0.50000066, 0.49999866, 0.50000024, 0.49999997, 0.5000003, 0.50000006, 0.49999917, 0.5000005, 0.50000113, 0.5000003, 0.49999997, 0.49999914, 0.5000013, 0.4999981, 0.49999917, 0.49999827, 0.49999964, 0.5000019, 0.4999983, 0.50000054, 0.49999952, 0.50000006, 0.49999943, 0.4999993, 0.49999854, 0.5000004, 0.49999872, 0.5000003, 0.50000125, 0.49999908, 0.5000001, 0.5000003, 0.5000013, 0.49999905, 0.49999917, 0.5000004, 0.500001, 0.5000021, 0.50000125, 0.50000155, 0.5000002, 0.50000006, 0.49999997, 0.49999896, 0.5000014, 0.50000006, 0.5000008, 0.49999806, 0.49999848, 0.499999, 0.5000023, 0.49999997, 0.49999964, 0.49999955, 0.5000001, 0.50000197, 0.5000003, 0.4999986, 0.4999999, 0.49999917, 0.4999995, 0.5000009, 0.49999943, 0.49999878, 0.49999937, 0.5000008, 0.5000013, 0.4999991, 0.49999997, 0.4999993, 0.49999985, 0.50000066, 0.49999914, 0.49999943];\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport matplotlib.pyplot as plt\\n\\n# Visualize the predicted images\\nfor i in range(2):  # Display the first two predicted images\\n    plt.figure()\\n    plt.imshow(y_pred[i].reshape(8, 16))  # Assuming each image has a shape of 8x16 pixels\\n    plt.title(f\"Predicted Image {i+1}\")\\n    plt.axis(\\'off\\')\\n    plt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CNN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Conv2D, Flatten, Dense, Reshape\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Load the dataset\n",
        "dataset_df = pd.read_csv('/content/dataImagesSum.csv')\n",
        "\n",
        "# Assuming X contains numerical features and y contains the image data\n",
        "X = dataset_df.iloc[:, 2:130].values\n",
        "y = dataset_df.iloc[:, 130:].values\n",
        "\n",
        "# Reshape y to be in the shape (number_of_samples, image_height, image_width, channels)\n",
        "# Since the images are likely to be grayscale as per the example, channels will be set to 1\n",
        "image_height, image_width = 8, 16  # Adjust these dimensions to match your actual image size\n",
        "y = y.reshape((-1, image_height, image_width, 1))\n",
        "\n",
        "# Manual split of the dataset\n",
        "X_test, y_test = X[:12, :], y[:12]\n",
        "X_train, y_train = X[12:, :], y[12:]\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(128,)))\n",
        "model.add(Reshape((16, 16, 1)))  # Reshape output to 2D format for convolution layers, adjust size as necessary\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(np.prod(y_train.shape[1:]), activation='sigmoid'))\n",
        "model.add(Reshape(y_train.shape[1:]))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "\n",
        "# Predict images from the test set features\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VEPS4amAc_q",
        "outputId": "b3d1e8e9-5eec-4158-f857-1c21d82bdafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "54/54 [==============================] - 4s 16ms/step - loss: 0.2407\n",
            "Epoch 2/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2312\n",
            "Epoch 3/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2254\n",
            "Epoch 4/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2262\n",
            "Epoch 5/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2167\n",
            "Epoch 6/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2215\n",
            "Epoch 7/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2131\n",
            "Epoch 8/50\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.2127\n",
            "Epoch 9/50\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2282\n",
            "Epoch 10/50\n",
            "54/54 [==============================] - 2s 32ms/step - loss: 0.2173\n",
            "Epoch 11/50\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2153\n",
            "Epoch 12/50\n",
            "54/54 [==============================] - 1s 24ms/step - loss: 0.2189\n",
            "Epoch 13/50\n",
            "54/54 [==============================] - 1s 20ms/step - loss: 0.2179\n",
            "Epoch 14/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2134\n",
            "Epoch 15/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2307\n",
            "Epoch 16/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2239\n",
            "Epoch 17/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2108\n",
            "Epoch 18/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2116\n",
            "Epoch 19/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2139\n",
            "Epoch 20/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2098\n",
            "Epoch 21/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2143\n",
            "Epoch 22/50\n",
            "54/54 [==============================] - 1s 17ms/step - loss: 0.2090\n",
            "Epoch 23/50\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.2190\n",
            "Epoch 24/50\n",
            "54/54 [==============================] - 1s 23ms/step - loss: 0.2097\n",
            "Epoch 25/50\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.2105\n",
            "Epoch 26/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2120\n",
            "Epoch 27/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2122\n",
            "Epoch 28/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2114\n",
            "Epoch 29/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2208\n",
            "Epoch 30/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2131\n",
            "Epoch 31/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2103\n",
            "Epoch 32/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2105\n",
            "Epoch 33/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2103\n",
            "Epoch 34/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2093\n",
            "Epoch 35/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2098\n",
            "Epoch 36/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2110\n",
            "Epoch 37/50\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.2132\n",
            "Epoch 38/50\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.2095\n",
            "Epoch 39/50\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.2129\n",
            "Epoch 40/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2264\n",
            "Epoch 41/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2254\n",
            "Epoch 42/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2088\n",
            "Epoch 43/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2091\n",
            "Epoch 44/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2114\n",
            "Epoch 45/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2097\n",
            "Epoch 46/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2105\n",
            "Epoch 47/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2132\n",
            "Epoch 48/50\n",
            "54/54 [==============================] - 1s 16ms/step - loss: 0.2120\n",
            "Epoch 49/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2106\n",
            "Epoch 50/50\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.2076\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1635\n",
            "Test Loss: 0.16350463032722473\n",
            "1/1 [==============================] - 0s 107ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, UpSampling2D, Conv2DTranspose, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from transformers import ViTModel, ViTConfig\n",
        "\n",
        "# Load the dataset\n",
        "dataset_df = pd.read_csv('/content/dataImagesSum.csv')\n",
        "\n",
        "# Assuming X contains numerical features and y contains the image data\n",
        "# Simulated data for demonstration\n",
        "X = np.random.rand(88, 32, 32, 3)  # Simulate 88 RGB images of shape 32x32\n",
        "y = np.random.rand(88, 64, 64, 1)  # Target images are 64x64 grayscale\n",
        "\n",
        "# Manual split of the dataset\n",
        "X_test, y_test = X[:12, :], y[:12]\n",
        "X_train, y_train = X[12:, :], y[12:]\n",
        "\n",
        "# Load the base model with pre-trained weights\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "inputs = base_model.input\n",
        "x = base_model(inputs)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Reshape((8, 8, 4))(x)  # Adjust dimensions as necessary\n",
        "\n",
        "# Upsample to target size\n",
        "x = UpSampling2D(size=(8, 8))(x)\n",
        "x = Conv2DTranspose(32, (3, 3), padding='same', activation='relu')(x)  # Use padding='same' to maintain size\n",
        "x = Conv2DTranspose(1, (3, 3), padding='same', activation='sigmoid')(x)  # Final layer for grayscale image\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs, x)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=1, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "\n",
        "# Predict images from the test set features\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHCi390hF5-1",
        "outputId": "c1158958-1e34-476f-e013-5baaf0faf9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - 13s 54ms/step - loss: 0.0833 - val_loss: 0.0833\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - 2s 36ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - 2s 36ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - 2s 28ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - 2s 31ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - 3s 37ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - 3s 38ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - 2s 27ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - 2s 36ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - 2s 25ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - 3s 38ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - 2s 32ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - 2s 24ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - 2s 26ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - 2s 35ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - 4s 54ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - 4s 63ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - 6s 86ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - 4s 52ms/step - loss: 0.0832 - val_loss: 0.0833\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0829\n",
            "Test Loss: 0.08285445719957352\n",
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        }
      ]
    }
  ]
}